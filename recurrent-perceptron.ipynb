{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecurLayer:\n",
    "    \n",
    "    # n_iter controls internal recurrence within one sample. When it is 0, no internal recurrence.\n",
    "    # External recurrence always happens between samples, if they are forwarded consecutively.\n",
    "    # Backwarding/updating will reset the context.\n",
    "    def __init__(self, n_x, n_h, n_iter=1, acti=\"TANH\"):\n",
    "        assert n_iter\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h        \n",
    "        \n",
    "        self.wx = np.random.uniform(-1, 1, (n_h, n_x)) * 0.1       \n",
    "        self.wh = np.ones(shape=(n_h, n_h)) \n",
    "        self.bh = np.random.uniform(-1, 1, (n_h, 1)) * 0.1\n",
    "\n",
    "        # for debugging purpose\n",
    "        #self.wx = np.ones(shape=(n_h, n_x))       \n",
    "        #self.wh = np.ones(shape=(n_h, n_h)) \n",
    "        #self.bh = np.ones(shape=(n_h, 1)) \n",
    "        \n",
    "        self.acti = Activation(acti)\n",
    "        self.n_iter = n_iter\n",
    "        self.n_recur = None\n",
    "        # initialize input/output and gradients data structure\n",
    "        self.reset_context()\n",
    "        \n",
    "    #              bh\n",
    "    # x -> (wx) -> + -> a -> (acti) -> -> h     \n",
    "    #              L- <-(wh)---------J \n",
    "    #                                   \n",
    "    def forward_1sample(self, x):\n",
    "        #set_trace()\n",
    "        assert self.n_x == x.size\n",
    "        self.X.append(x)\n",
    "        \n",
    "        s = np.dot(self.wx, x) \n",
    "        h = self.H[-1][-1]\n",
    "        local_H = [] \n",
    "        # internal recurrence forward\n",
    "        for i in range(self.n_iter):\n",
    "            a = s + np.dot(self.wh, h) + self.bh\n",
    "            h = self.acti.func(a)\n",
    "            local_H.append(h)\n",
    "        \n",
    "        self.H.append(local_H)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, nx):\n",
    "        self.n_recur = nx.shape[0]\n",
    "        # external recurrent forward\n",
    "        for i in range(self.n_recur):\n",
    "            h = self.forward_1sample(nx[i])\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def backward_1sample(self, g_h):\n",
    "        #set_trace()\n",
    "        assert g_h.size == self.n_h\n",
    "        x = self.X.pop()\n",
    "        local_H = self.H.pop()\n",
    "        # internal recurrence backward\n",
    "        for i in reversed(range(self.n_iter)):\n",
    "            h = local_H.pop()\n",
    "            g_signal = self.acti.grad(h)\n",
    "            g_a = g_signal * g_h\n",
    "            self.g_bh += g_a\n",
    "            self.g_wh += np.outer(g_a, h)\n",
    "            self.g_wx += np.outer(g_a, x)\n",
    "            self.g_x += np.dot(self.wx.T, g_a)\n",
    "            g_h = np.dot(self.wh.T, g_a)\n",
    "        \n",
    "        return g_h\n",
    "\n",
    "    def backward(self, g_h):\n",
    "        assert g_h.size == self.n_h\n",
    "        # external recurrent backward\n",
    "        self.g_bh = self.g_wh = self.g_wx = self.g_x = 0\n",
    "        for i in range(self.n_recur):\n",
    "            g_h = self.backward_1sample(g_h)\n",
    "        \n",
    "        return self.g_x\n",
    "        \n",
    "    def update(self, learning=0.01):\n",
    "        self.wx -= self.g_wx * learning\n",
    "        self.wh -= self.g_wh * learning\n",
    "        self.bh -= self.g_bh * learning\n",
    "        init_h = np.zeros(shape=(self.n_h, 1))\n",
    "        orig_h = self.H.pop().pop()\n",
    "        assert (orig_h == init_h).all() and self.H == []\n",
    "        self.reset_context()\n",
    "        return\n",
    "    \n",
    "    def reset_context(self):\n",
    "        self.X = []\n",
    "        init_h = np.zeros(shape=(self.n_h, 1))\n",
    "        self.H = [[init_h]]\n",
    "        self.g_bh = self.g_wh = self.g_wx = self.g_x = 0\n",
    "        self.n_recur = None\n",
    "        return\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"\\nX is:\\n\"+str(self.X)\n",
    "        s += \"\\nwx is:\\n\" + str(self.wx)\n",
    "        s += \"\\nwh is:\\n\" + str(self.wh)\n",
    "        s += \"\\nbh is:\\n\" + str(self.bh)\n",
    "        s += \"\\nH is:\\n\"+str(self.H)\n",
    "        s += \"\\ng_H is:\\n\"+str(self.g_h)\n",
    "        s += \"\\ng_wx is:\\n\"+str(self.g_wx)\n",
    "        s += \"\\ng_wh is:\\n\"+str(self.g_wh)\n",
    "        s += \"\\ng_bh is:\\n\"+str(self.g_bh)\n",
    "        s += \"\\ng_X is:\\n\"+str(self.g_x)\n",
    "        \n",
    "        return s\n",
    "        \n",
    "class RecurNet:\n",
    "    \n",
    "    def __init__(self, n_x, n_h, n_y, h_acti=\"TANH\", y_acti=\"RELU\", \n",
    "                 n_iter=1, outlayer=None, learning=0.01):\n",
    "\n",
    "        n_h = n_h if n_h > 0 else (n_x + n_y) // 2\n",
    "        self.n_x = n_x\n",
    "        self.recur = RecurLayer(n_x, n_h, n_iter, h_acti)\n",
    "        self.output = outlayer\n",
    "        if outlayer is None:\n",
    "            self.output = PercepLayer(n_h, n_y, y_acti)\n",
    "        \n",
    "        self.learning = learning\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.recur.forward(x)\n",
    "        y = self.output.forward(h)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, g_y):\n",
    "        g_h = self.output.backward(g_y)\n",
    "        g_x = self.recur.backward(g_h)\n",
    "        return g_x\n",
    "\n",
    "    def update(self):\n",
    "        self.output.update(self.learning)\n",
    "        self.recur.update(self.learning)\n",
    "        return\n",
    "    \n",
    "    def predict_1sample(self, x):\n",
    "        n_recur = x.size // self.n_x\n",
    "        x = x.reshape(n_recur, self.n_x, 1)\n",
    "        y = self.forward(x)\n",
    "        self.recur.reset_context()\n",
    "        return y\n",
    "    \n",
    "    def train_1sample(self, x, label):\n",
    "        #set_trace()\n",
    "        n_recur = x.size // self.n_x\n",
    "        x = x.reshape(n_recur, self.n_x, 1)\n",
    "        self.forward(x)\n",
    "        self.backward(label)\n",
    "        self.update()\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'multilayer-perceptron.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predict:  [1 2 3] 2\n",
      "\n",
      "Predict:  [2 3 1] 1\n",
      "\n",
      "Predict:  [3 1 2] 0\n"
     ]
    }
   ],
   "source": [
    "def run_rnn_test():\n",
    "    n_x=3; n_h=4; n_class=3\n",
    "    outlayer = SoftMaxLayer(n_h, n_class)\n",
    "    rnn = RecurNet(n_x, n_h, n_class, n_iter=1, outlayer=outlayer, h_acti=\"SIGMOID\", learning=0.1)\n",
    "    X = [\n",
    "        [1,2,3],\n",
    "        [2,1,3],\n",
    "        [3,1,2],\n",
    "        [3,2,1],\n",
    "        [1,3,2],\n",
    "        [2,3,1]\n",
    "        ]\n",
    "    Y = [2,2,0,0,1,1]\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    for j in range(40):\n",
    "#        if j % 10 == 0:\n",
    "#            print(rnn.recur)\n",
    "#            print(rnn.output)\n",
    "            \n",
    "        for i in range(X.shape[0]):\n",
    "            rnn.train_1sample(X[i].reshape(-1,1), Y[i])\n",
    "\n",
    "    X = np.array([[1,2,3],[2,3,1],[3,1,2]])\n",
    "    for i in range(X.shape[0]):\n",
    "        predict = rnn.predict_1sample(X[i].reshape(-1,1))\n",
    "        print(\"\\nPredict: \", X[i], predict)\n",
    "\n",
    "if __name__  == '__main__':\n",
    "    run_rnn_test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of epoch 0 is 0.955\n",
      "\n",
      "Accuracy of epoch 1 is 0.9622\n",
      "\n",
      "Accuracy of epoch 2 is 0.9681\n",
      "\n",
      "Accuracy of epoch 3 is 0.97\n",
      "\n",
      "Accuracy of epoch 4 is 0.9728\n"
     ]
    }
   ],
   "source": [
    "%run 'mnist.ipynb'\n",
    "def run_rnn_mnist():\n",
    "    n_x = 28; n_h= 28*3+1; n_class=10\n",
    "    outlayer = SoftMaxLayer(n_h, n_class)\n",
    "    rnn = RecurNet(n_x, n_h, n_class, n_iter=1,\n",
    "                   outlayer=outlayer, h_acti=\"RELU\", learning=0.001)\n",
    "    mnist = MNIST(rnn, folder=\"../convolution-network\")\n",
    "    for i in range(5):\n",
    "        mnist.train(-1)\n",
    "        accuracy = mnist.test(-1)\n",
    "        print(\"\\nAccuracy of epoch {} is {}\".format(i, accuracy))\n",
    "    return mnist\n",
    "\n",
    "mnist = None\n",
    "if __name__  == '__main__':\n",
    "    mnist = run_mlp_mnist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
